{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[  0,   1,   2,   3,   4],\n",
      "          [ 15,  16,  17,  18,  19],\n",
      "          [ 30,  31,  32,  33,  34],\n",
      "          [ 45,  46,  47,  48,  49],\n",
      "          [ 60,  61,  62,  63,  64],\n",
      "          [ 75,  76,  77,  78,  79],\n",
      "          [ 90,  91,  92,  93,  94],\n",
      "          [105, 106, 107, 108, 109],\n",
      "          [120, 121, 122, 123, 124],\n",
      "          [135, 136, 137, 138, 139],\n",
      "          [150, 151, 152, 153, 154],\n",
      "          [165, 166, 167, 168, 169],\n",
      "          [180, 181, 182, 183, 184],\n",
      "          [195, 196, 197, 198, 199],\n",
      "          [210, 211, 212, 213, 214]],\n",
      "\n",
      "         [[225, 226, 227, 228, 229],\n",
      "          [240, 241, 242, 243, 244],\n",
      "          [255, 256, 257, 258, 259],\n",
      "          [270, 271, 272, 273, 274],\n",
      "          [285, 286, 287, 288, 289],\n",
      "          [300, 301, 302, 303, 304],\n",
      "          [315, 316, 317, 318, 319],\n",
      "          [330, 331, 332, 333, 334],\n",
      "          [345, 346, 347, 348, 349],\n",
      "          [360, 361, 362, 363, 364],\n",
      "          [375, 376, 377, 378, 379],\n",
      "          [390, 391, 392, 393, 394],\n",
      "          [405, 406, 407, 408, 409],\n",
      "          [420, 421, 422, 423, 424],\n",
      "          [435, 436, 437, 438, 439]],\n",
      "\n",
      "         [[450, 451, 452, 453, 454],\n",
      "          [465, 466, 467, 468, 469],\n",
      "          [480, 481, 482, 483, 484],\n",
      "          [495, 496, 497, 498, 499],\n",
      "          [510, 511, 512, 513, 514],\n",
      "          [525, 526, 527, 528, 529],\n",
      "          [540, 541, 542, 543, 544],\n",
      "          [555, 556, 557, 558, 559],\n",
      "          [570, 571, 572, 573, 574],\n",
      "          [585, 586, 587, 588, 589],\n",
      "          [600, 601, 602, 603, 604],\n",
      "          [615, 616, 617, 618, 619],\n",
      "          [630, 631, 632, 633, 634],\n",
      "          [645, 646, 647, 648, 649],\n",
      "          [660, 661, 662, 663, 664]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.arange(1*3*15*15).view(1,3,15,15)\n",
    "# print(a)\n",
    "# print(a[:, 1:2])\n",
    "# print(torch.chunk(a, 4, dim=1)[0][:, :3, :])\n",
    "# # b = torch.ones(3, 4, 4)\n",
    "print(a[:, :, :, 0:5])\n",
    "# # c = torch.cat([a, b[:, :3, :]], dim=1)\n",
    "# # print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor([[[[  0,   1,   2,   3],\n",
      "          [ 15,  16,  17,  18],\n",
      "          [ 30,  31,  32,  33],\n",
      "          [ 45,  46,  47,  48],\n",
      "          [ 60,  61,  62,  63],\n",
      "          [ 75,  76,  77,  78],\n",
      "          [ 90,  91,  92,  93],\n",
      "          [105, 106, 107, 108],\n",
      "          [120, 121, 122, 123],\n",
      "          [135, 136, 137, 138],\n",
      "          [150, 151, 152, 153],\n",
      "          [165, 166, 167, 168],\n",
      "          [180, 181, 182, 183],\n",
      "          [195, 196, 197, 198],\n",
      "          [210, 211, 212, 213]],\n",
      "\n",
      "         [[225, 226, 227, 228],\n",
      "          [240, 241, 242, 243],\n",
      "          [255, 256, 257, 258],\n",
      "          [270, 271, 272, 273],\n",
      "          [285, 286, 287, 288],\n",
      "          [300, 301, 302, 303],\n",
      "          [315, 316, 317, 318],\n",
      "          [330, 331, 332, 333],\n",
      "          [345, 346, 347, 348],\n",
      "          [360, 361, 362, 363],\n",
      "          [375, 376, 377, 378],\n",
      "          [390, 391, 392, 393],\n",
      "          [405, 406, 407, 408],\n",
      "          [420, 421, 422, 423],\n",
      "          [435, 436, 437, 438]],\n",
      "\n",
      "         [[450, 451, 452, 453],\n",
      "          [465, 466, 467, 468],\n",
      "          [480, 481, 482, 483],\n",
      "          [495, 496, 497, 498],\n",
      "          [510, 511, 512, 513],\n",
      "          [525, 526, 527, 528],\n",
      "          [540, 541, 542, 543],\n",
      "          [555, 556, 557, 558],\n",
      "          [570, 571, 572, 573],\n",
      "          [585, 586, 587, 588],\n",
      "          [600, 601, 602, 603],\n",
      "          [615, 616, 617, 618],\n",
      "          [630, 631, 632, 633],\n",
      "          [645, 646, 647, 648],\n",
      "          [660, 661, 662, 663]]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ensure_divisibility(numerator, denominator):\n",
    "    \"\"\"Ensure that numerator is divisible by the denominator.\"\"\"\n",
    "    assert numerator % denominator == 0, '{} is not divisible by {}'.format(\n",
    "        numerator, denominator)\n",
    "\n",
    "\n",
    "def divide(numerator, denominator):\n",
    "    \"\"\"Ensure that numerator is divisible by the denominator and return\n",
    "    the division value.\"\"\"\n",
    "    # ensure_divisibility(numerator, denominator)\n",
    "    return numerator // denominator\n",
    "\n",
    "def split_tensor_along_last_dim(tensor, num_partitions,\n",
    "                                kernel_size,\n",
    "                                contiguous_split_chunks=False):\n",
    "    \"\"\"Split a tensor along its last dimension.\n",
    "    Arguments:\n",
    "        tensor: input tensor.\n",
    "        num_partitions: number of partitions to split the tensor\n",
    "        contiguous_split_chunks: If True, make each chunk contiguous\n",
    "                                 in memory.\n",
    "    \"\"\"\n",
    "    # Get the size and dimension.\n",
    "    last_dim = tensor.dim() - 1\n",
    "    last_dim_size = divide(tensor.size()[last_dim], num_partitions)\n",
    "    print(last_dim_size)\n",
    "    # Split.\n",
    "    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\n",
    "    tensor_custom = [torch.cat([tensor_list[i], tensor[:, :, :, ((i+1)*last_dim_size):((i+1)*last_dim_size)+kernel_size-1]], dim=last_dim) for i in range(num_partitions-1)]\n",
    "    tensor_custom.append(tensor_list[-1])\n",
    "\n",
    "    # Note: torch.split does not create contiguous tensors by default.\n",
    "    if contiguous_split_chunks:\n",
    "        return tuple(chunk.contiguous() for chunk in tensor_list)\n",
    "\n",
    "    return tensor_custom\n",
    "\n",
    "\n",
    "print(split_tensor_along_last_dim(a, 4, 2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27212/580500247.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27212/580500247.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m                                 bias=False)\n\u001b[0;32m     55\u001b[0m     parallel_conv = ParallelConv2d(3, 64, kernel_size=7, stride=2, padding=3,\n\u001b[1;32m---> 56\u001b[1;33m                                 bias=False)\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[0mserial_conv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mserial_conv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparallel_conv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Distribution_example\\model\\resnet.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, last_cnn, bias, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m                  padding=0, dilation=1, groups=1, last_cnn=False, bias=True, **kwargs):\n\u001b[0;32m     50\u001b[0m         super(ParallelConv2d, self).__init__(in_channels, out_channels, kernel_size, stride,\n\u001b[1;32m---> 51\u001b[1;33m                  padding, dilation, groups, bias, device=torch.cuda.current_device(), **kwargs)\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mngpus_per_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from model.resnet import ParallelConv2d\n",
    "from model.initialize import initialize_model_parallel\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "\n",
    "def test_conv(rank, ngpus_per_node, serial_conv, parallel_conv, input_data):\n",
    "    if rank():\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "\n",
    "    end.record()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    dist.init_process_group(backend=\"nccl\", init_method=\"tcp://127.0.0.1:6006\", world_size=ngpus_per_node, rank=rank)\n",
    "    initialize_model_parallel()\n",
    "    for p in parallel_conv.parameters():\n",
    "        print(\"[ rank {} ] parallel weight : \".format(str(rank)), p)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    parallel_conv.cuda(rank)\n",
    "    if rank == 0:\n",
    "        serial_conv = torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                bias=False).cuda(rank)\n",
    "        for p in serial_conv.parameters():\n",
    "            print(\"serial weight :\",  p)\n",
    "\n",
    "    parallel_result = parallel_conv(parallel_test_data)\n",
    "\n",
    "    if rank == 0:\n",
    "        print(\"[Device : {} ]\".format(str(torch.cuda.current_device())))\n",
    "        serial_result = serial_conv(parallel_test_data)\n",
    "        print(\"Parallel Result Shape : \", parallel_result.size())\n",
    "        print(\"Serial Result Shape : \", serial_result.size())\n",
    "        print(\"Parallel : \", parallel_result)\n",
    "        print(\"Serial : \", serial_result)\n",
    "        # parallel_result and serial_result should be the same\n",
    "        assert torch.allclose(parallel_result, serial_result), \"Parallel and Serial results are not the same\"\n",
    "        print(\"Conv layer Test Passed!!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 한 노드의 GPU수를 가져옴.\n",
    "    ngpus_per_node = torch.cuda.device_count()\n",
    "    input_data = torch.randn(1, 3, 224, 224)\n",
    "    # multiprocessing_distributed 변수가 true라면, world_size를 총 GPU개수로 설정한 후에, 메인 워커를 실행함.\n",
    "    serial_conv = torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                bias=False)\n",
    "    parallel_conv = ParallelConv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                bias=False)\n",
    "    serial_conv.weight = torch.ones((2,), dtype=torch.int8)\n",
    "    serial_conv.weight.new_tensor(parallel_conv.weight)\n",
    "    print(\"Are they same? : \", torch.allclose(serial_conv.weight, parallel_conv.weight))\n",
    "\n",
    "    #mp.spawn(test_conv, nprocs=ngpus_per_node, args=(ngpus_per_node, serial_conv, parallel_conv, input_data))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0753,  0.0312,  0.0084,  ...,  0.0498,  0.0002,  0.0824],\n",
      "          [ 0.0799, -0.0630, -0.0130,  ...,  0.0809,  0.0636,  0.0688],\n",
      "          [-0.0088, -0.0663,  0.0739,  ..., -0.0795, -0.0810,  0.0557],\n",
      "          ...,\n",
      "          [-0.0821, -0.0409, -0.0492,  ...,  0.0804,  0.0298,  0.0719],\n",
      "          [-0.0641, -0.0250,  0.0629,  ..., -0.0809,  0.0558, -0.0650],\n",
      "          [-0.0066,  0.0691,  0.0639,  ...,  0.0469,  0.0375,  0.0584]],\n",
      "\n",
      "         [[-0.0543,  0.0693, -0.0768,  ..., -0.0251,  0.0316,  0.0338],\n",
      "          [-0.0789, -0.0232, -0.0717,  ..., -0.0712,  0.0341, -0.0531],\n",
      "          [ 0.0211, -0.0073, -0.0616,  ..., -0.0237,  0.0784,  0.0252],\n",
      "          ...,\n",
      "          [-0.0634, -0.0019, -0.0616,  ...,  0.0016, -0.0012,  0.0531],\n",
      "          [-0.0738, -0.0161, -0.0709,  ...,  0.0176,  0.0616,  0.0707],\n",
      "          [-0.0629,  0.0606,  0.0364,  ..., -0.0052,  0.0097, -0.0705]],\n",
      "\n",
      "         [[-0.0606,  0.0284, -0.0641,  ..., -0.0329, -0.0711, -0.0482],\n",
      "          [-0.0144,  0.0051, -0.0031,  ...,  0.0489,  0.0684, -0.0280],\n",
      "          [-0.0779, -0.0698,  0.0780,  ..., -0.0729, -0.0202,  0.0125],\n",
      "          ...,\n",
      "          [-0.0600, -0.0429, -0.0787,  ..., -0.0012,  0.0110, -0.0792],\n",
      "          [ 0.0696,  0.0351,  0.0670,  ...,  0.0674, -0.0696,  0.0693],\n",
      "          [ 0.0491,  0.0807, -0.0357,  ..., -0.0152,  0.0789, -0.0502]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0070, -0.0438,  0.0733,  ...,  0.0528, -0.0365, -0.0734],\n",
      "          [ 0.0093,  0.0663, -0.0447,  ...,  0.0824,  0.0624, -0.0498],\n",
      "          [-0.0076, -0.0691,  0.0224,  ..., -0.0004,  0.0297, -0.0504],\n",
      "          ...,\n",
      "          [ 0.0630,  0.0117, -0.0241,  ..., -0.0299,  0.0371,  0.0612],\n",
      "          [-0.0765,  0.0474, -0.0385,  ..., -0.0455, -0.0346,  0.0312],\n",
      "          [ 0.0755, -0.0477, -0.0551,  ..., -0.0631, -0.0222, -0.0550]],\n",
      "\n",
      "         [[ 0.0263,  0.0443,  0.0143,  ..., -0.0612, -0.0054, -0.0249],\n",
      "          [-0.0442,  0.0221,  0.0303,  ...,  0.0473,  0.0815, -0.0460],\n",
      "          [-0.0173,  0.0102,  0.0124,  ...,  0.0491, -0.0419,  0.0258],\n",
      "          ...,\n",
      "          [-0.0493, -0.0550, -0.0387,  ...,  0.0466, -0.0211, -0.0056],\n",
      "          [ 0.0183, -0.0539, -0.0396,  ..., -0.0060, -0.0801,  0.0309],\n",
      "          [ 0.0239, -0.0412, -0.0550,  ..., -0.0325,  0.0700,  0.0420]],\n",
      "\n",
      "         [[-0.0544, -0.0313, -0.0085,  ..., -0.0770,  0.0052, -0.0316],\n",
      "          [ 0.0533,  0.0813,  0.0564,  ..., -0.0495, -0.0344,  0.0398],\n",
      "          [-0.0761,  0.0349, -0.0821,  ...,  0.0467,  0.0498, -0.0663],\n",
      "          ...,\n",
      "          [ 0.0446,  0.0573, -0.0716,  ...,  0.0744,  0.0515, -0.0654],\n",
      "          [-0.0101,  0.0679,  0.0175,  ...,  0.0663,  0.0030,  0.0660],\n",
      "          [-0.0771,  0.0517,  0.0301,  ...,  0.0587, -0.0809, -0.0769]]],\n",
      "\n",
      "\n",
      "        [[[-0.0273,  0.0210, -0.0511,  ...,  0.0775, -0.0285, -0.0287],\n",
      "          [ 0.0124,  0.0437,  0.0024,  ..., -0.0478, -0.0004,  0.0352],\n",
      "          [-0.0623, -0.0708, -0.0528,  ...,  0.0122, -0.0481,  0.0683],\n",
      "          ...,\n",
      "          [-0.0421, -0.0756, -0.0441,  ..., -0.0609,  0.0248, -0.0395],\n",
      "          [ 0.0720,  0.0598,  0.0767,  ..., -0.0381,  0.0397,  0.0716],\n",
      "          [ 0.0769, -0.0801,  0.0682,  ..., -0.0562, -0.0680,  0.0560]],\n",
      "\n",
      "         [[ 0.0050, -0.0125,  0.0010,  ...,  0.0614, -0.0020,  0.0166],\n",
      "          [ 0.0564, -0.0443,  0.0641,  ...,  0.0482, -0.0665, -0.0123],\n",
      "          [-0.0083, -0.0208,  0.0704,  ...,  0.0206, -0.0591,  0.0159],\n",
      "          ...,\n",
      "          [-0.0125, -0.0725, -0.0150,  ..., -0.0065,  0.0104,  0.0621],\n",
      "          [-0.0497,  0.0589,  0.0464,  ...,  0.0382, -0.0418, -0.0062],\n",
      "          [-0.0297,  0.0449, -0.0659,  ..., -0.0109, -0.0056, -0.0051]],\n",
      "\n",
      "         [[ 0.0018, -0.0066,  0.0030,  ...,  0.0623,  0.0763, -0.0276],\n",
      "          [ 0.0125,  0.0242, -0.0743,  ..., -0.0188,  0.0108,  0.0015],\n",
      "          [ 0.0236, -0.0600, -0.0169,  ...,  0.0324,  0.0048, -0.0126],\n",
      "          ...,\n",
      "          [ 0.0656, -0.0233,  0.0781,  ...,  0.0186, -0.0086, -0.0181],\n",
      "          [-0.0217, -0.0220,  0.0265,  ..., -0.0399,  0.0248,  0.0618],\n",
      "          [-0.0753,  0.0763,  0.0360,  ..., -0.0507, -0.0474,  0.0314]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0648,  0.0023,  0.0568,  ..., -0.0055, -0.0517, -0.0791],\n",
      "          [-0.0090, -0.0820,  0.0628,  ...,  0.0006, -0.0592,  0.0505],\n",
      "          [-0.0501,  0.0632,  0.0791,  ..., -0.0284, -0.0748,  0.0665],\n",
      "          ...,\n",
      "          [ 0.0078, -0.0764, -0.0137,  ...,  0.0405, -0.0193, -0.0410],\n",
      "          [ 0.0204, -0.0704, -0.0809,  ...,  0.0272,  0.0087, -0.0605],\n",
      "          [-0.0214, -0.0506, -0.0750,  ...,  0.0305,  0.0777,  0.0730]],\n",
      "\n",
      "         [[-0.0726, -0.0396, -0.0213,  ..., -0.0265,  0.0259,  0.0476],\n",
      "          [ 0.0516,  0.0684, -0.0652,  ...,  0.0022,  0.0758, -0.0130],\n",
      "          [ 0.0187,  0.0686,  0.0192,  ...,  0.0515, -0.0222, -0.0125],\n",
      "          ...,\n",
      "          [ 0.0533, -0.0311,  0.0129,  ..., -0.0669, -0.0129,  0.0691],\n",
      "          [ 0.0662, -0.0172,  0.0092,  ..., -0.0013,  0.0759, -0.0545],\n",
      "          [-0.0136, -0.0294,  0.0146,  ...,  0.0682,  0.0374, -0.0717]],\n",
      "\n",
      "         [[ 0.0531,  0.0586, -0.0276,  ...,  0.0616,  0.0673, -0.0164],\n",
      "          [ 0.0661,  0.0556, -0.0073,  ..., -0.0023, -0.0468,  0.0188],\n",
      "          [ 0.0705,  0.0517,  0.0316,  ...,  0.0058, -0.0019, -0.0205],\n",
      "          ...,\n",
      "          [-0.0080,  0.0022, -0.0144,  ...,  0.0377,  0.0168, -0.0715],\n",
      "          [ 0.0217,  0.0703,  0.0245,  ..., -0.0031,  0.0626,  0.0009],\n",
      "          [ 0.0348, -0.0333, -0.0138,  ...,  0.0129,  0.0497,  0.0701]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0661,  0.0534,  0.0116,  ..., -0.0667, -0.0437,  0.0804],\n",
      "          [ 0.0600, -0.0288, -0.0103,  ..., -0.0806, -0.0546, -0.0707],\n",
      "          [ 0.0366, -0.0642, -0.0322,  ..., -0.0538, -0.0483, -0.0720],\n",
      "          ...,\n",
      "          [ 0.0155,  0.0636,  0.0102,  ...,  0.0631, -0.0651,  0.0159],\n",
      "          [ 0.0643, -0.0764,  0.0570,  ...,  0.0596,  0.0207,  0.0538],\n",
      "          [-0.0045,  0.0770,  0.0428,  ...,  0.0460, -0.0378, -0.0712]],\n",
      "\n",
      "         [[-0.0572, -0.0667,  0.0459,  ...,  0.0259,  0.0462, -0.0340],\n",
      "          [ 0.0021,  0.0815, -0.0194,  ...,  0.0748, -0.0674, -0.0208],\n",
      "          [-0.0051, -0.0319,  0.0797,  ..., -0.0472,  0.0438, -0.0226],\n",
      "          ...,\n",
      "          [-0.0823,  0.0821,  0.0700,  ...,  0.0066, -0.0175,  0.0232],\n",
      "          [-0.0209, -0.0056,  0.0217,  ..., -0.0545, -0.0275,  0.0500],\n",
      "          [ 0.0755,  0.0030, -0.0066,  ...,  0.0361,  0.0311, -0.0556]],\n",
      "\n",
      "         [[-0.0467,  0.0400, -0.0131,  ...,  0.0618, -0.0050,  0.0182],\n",
      "          [-0.0480,  0.0406,  0.0547,  ...,  0.0113,  0.0021, -0.0515],\n",
      "          [ 0.0155, -0.0660,  0.0409,  ...,  0.0231,  0.0012,  0.0195],\n",
      "          ...,\n",
      "          [-0.0625,  0.0184, -0.0652,  ..., -0.0261,  0.0656,  0.0551],\n",
      "          [ 0.0238, -0.0305,  0.0625,  ..., -0.0566, -0.0819,  0.0101],\n",
      "          [-0.0364, -0.0798, -0.0263,  ..., -0.0681,  0.0527, -0.0685]]],\n",
      "\n",
      "\n",
      "        [[[-0.0341,  0.0722, -0.0575,  ..., -0.0333, -0.0796, -0.0618],\n",
      "          [ 0.0263,  0.0216,  0.0300,  ..., -0.0294,  0.0333, -0.0105],\n",
      "          [-0.0599, -0.0647, -0.0505,  ...,  0.0556,  0.0722,  0.0089],\n",
      "          ...,\n",
      "          [ 0.0492, -0.0652,  0.0283,  ..., -0.0673, -0.0212,  0.0794],\n",
      "          [ 0.0385,  0.0540,  0.0570,  ..., -0.0818, -0.0566, -0.0247],\n",
      "          [-0.0706,  0.0588,  0.0081,  ...,  0.0630,  0.0428, -0.0810]],\n",
      "\n",
      "         [[-0.0078,  0.0324,  0.0230,  ...,  0.0420,  0.0394,  0.0299],\n",
      "          [-0.0043, -0.0100, -0.0022,  ...,  0.0318,  0.0775, -0.0038],\n",
      "          [-0.0325,  0.0502, -0.0455,  ...,  0.0821,  0.0365, -0.0732],\n",
      "          ...,\n",
      "          [ 0.0498, -0.0608,  0.0715,  ..., -0.0614,  0.0593,  0.0262],\n",
      "          [ 0.0289, -0.0059, -0.0018,  ..., -0.0239,  0.0518, -0.0740],\n",
      "          [-0.0032,  0.0136, -0.0125,  ..., -0.0338, -0.0516,  0.0234]],\n",
      "\n",
      "         [[ 0.0591,  0.0621, -0.0115,  ..., -0.0339, -0.0593, -0.0736],\n",
      "          [-0.0810,  0.0524,  0.0024,  ..., -0.0393, -0.0544, -0.0523],\n",
      "          [-0.0597,  0.0021,  0.0659,  ...,  0.0265,  0.0652,  0.0474],\n",
      "          ...,\n",
      "          [ 0.0074, -0.0245, -0.0625,  ..., -0.0322, -0.0134, -0.0468],\n",
      "          [ 0.0362, -0.0026,  0.0425,  ..., -0.0092,  0.0056,  0.0681],\n",
      "          [ 0.0391,  0.0108, -0.0093,  ..., -0.0449, -0.0591, -0.0610]]]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "serial_conv = torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                bias=False)\n",
    "serial_conv1 = torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                bias=False)\n",
    "\n",
    "\n",
    "serial_conv.weight = serial_conv1.weight\n",
    "\n",
    "print(torch.allclose(serial_conv.weight, serial_conv1.weight))\n",
    "parallel_conv = torch.empty_like(serial_conv.weight).copy_(serial_conv.weight)\n",
    "\n",
    "print(torch.allclose(serial_conv.weight, serial_conv1.weight))\n",
    "print(serial_conv.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - plane data tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5089e-01, 9.4022e-01, 6.2946e-01,\n",
      "        1.7595e-02, 3.2779e-01, 6.3036e-01, 1.6631e-01, 8.4149e-01, 1.0299e-01,\n",
      "        5.8099e-01, 9.4549e-02, 3.5169e-01, 4.5150e-02, 3.1739e-01, 1.9495e-01,\n",
      "        6.2202e-01, 1.9836e-01, 3.7513e-01, 9.9551e-01, 3.7389e-01, 1.2062e-01,\n",
      "        4.2645e-01, 1.7901e-01, 9.6288e-01, 5.0867e-01, 1.2028e-01, 8.8518e-01,\n",
      "        4.7485e-01, 2.8794e-01, 7.3332e-01, 4.8651e-01, 1.4022e-01, 4.2407e-01,\n",
      "        6.7777e-01, 3.4144e-01, 2.6838e-01, 1.5350e-02, 7.2247e-01, 4.6532e-02,\n",
      "        3.7014e-01, 4.9245e-01, 9.9718e-01, 3.0482e-01, 5.0409e-01, 8.4973e-01,\n",
      "        6.1963e-02, 9.8711e-01, 9.7834e-01, 7.7821e-01, 7.2188e-02, 3.4646e-01,\n",
      "        5.3620e-01, 8.7090e-01, 4.9958e-01, 2.8303e-01, 8.5810e-01, 8.3190e-01,\n",
      "        6.6031e-01, 8.2808e-01, 6.9888e-01, 2.2070e-01, 3.4924e-01, 8.7521e-01,\n",
      "        1.8198e-01, 7.6968e-01, 8.9894e-01, 8.5397e-01, 6.8809e-01, 9.4523e-02,\n",
      "        4.3997e-01, 3.9607e-01, 8.3872e-03, 1.4808e-02, 7.8272e-01, 8.7690e-01,\n",
      "        1.5037e-01, 8.5996e-01, 7.7921e-01, 5.1543e-01, 5.9001e-01, 5.8517e-02,\n",
      "        1.6269e-01, 4.7455e-01, 5.3517e-01, 5.9622e-01, 1.3580e-01, 1.1249e-01,\n",
      "        3.3316e-01, 1.1852e-01, 3.7903e-01, 4.0628e-01, 8.2985e-01, 9.0706e-01,\n",
      "        2.1196e-01, 8.8883e-01, 9.7382e-01, 9.2784e-01, 5.0497e-01, 8.3488e-01,\n",
      "        2.5588e-01, 6.2599e-01, 3.8899e-02, 8.2554e-01, 9.7194e-01, 4.0699e-01,\n",
      "        2.9608e-01, 8.3885e-01, 7.6296e-01, 5.3827e-01, 7.0693e-01, 2.8621e-01,\n",
      "        1.2195e-01, 9.6711e-01, 5.3602e-04, 5.4843e-01, 1.7697e-01, 2.8856e-02,\n",
      "        3.9190e-01, 5.9670e-02, 4.5230e-01, 9.2800e-01, 9.3211e-02, 1.0267e-01,\n",
      "        2.0840e-01, 8.1723e-01, 6.2664e-02, 4.5246e-01, 7.4320e-01, 4.0748e-01,\n",
      "        3.6369e-01, 9.1750e-01, 2.7624e-01, 5.2549e-02, 9.3358e-01, 3.9317e-01,\n",
      "        4.3075e-01, 3.7489e-01, 1.1421e-01, 7.0649e-01, 2.9446e-01, 8.1633e-01,\n",
      "        3.4806e-01, 1.9906e-02, 9.5522e-01, 9.8853e-01, 4.4761e-01, 9.9027e-01,\n",
      "        5.5884e-01, 7.1895e-01, 4.3565e-01, 5.8336e-01, 9.9298e-01, 2.0295e-01,\n",
      "        4.4612e-01, 3.6332e-01, 8.3499e-01, 6.8588e-01, 8.9052e-02, 7.4130e-01,\n",
      "        4.2362e-01, 1.9561e-01, 7.2612e-01, 4.6932e-01, 6.1458e-01, 8.3057e-01,\n",
      "        4.9735e-01, 3.4275e-01, 9.0038e-01, 1.3636e-01, 1.4621e-02, 4.6450e-01,\n",
      "        3.0965e-01, 3.3253e-01, 5.2106e-01, 5.6447e-01, 1.0104e-02, 7.2618e-01,\n",
      "        1.1078e-01, 6.3328e-01, 3.8121e-01, 2.0814e-01, 3.8563e-01, 5.3916e-01,\n",
      "        7.3859e-01, 2.2972e-01, 4.3517e-01, 5.0551e-01, 2.0089e-01, 9.8999e-01,\n",
      "        3.9771e-01, 6.6274e-01, 7.3624e-01, 2.2023e-01, 5.4758e-01, 3.9593e-01,\n",
      "        4.8836e-01, 3.0355e-01, 1.6788e-01, 1.4900e-01, 3.3707e-01, 9.3674e-01,\n",
      "        4.9667e-01, 5.6455e-01, 2.6956e-01, 6.0404e-01, 6.2415e-01, 1.3064e-01,\n",
      "        7.2258e-01, 7.0882e-01, 1.5159e-01, 6.2318e-02, 5.3707e-01, 7.9292e-01,\n",
      "        8.9327e-01, 2.0463e-01, 7.0041e-01, 1.3349e-01, 5.9233e-01, 1.5991e-01,\n",
      "        4.3561e-01, 3.2397e-01, 2.6631e-01, 8.2425e-01, 8.5740e-01, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00])\n",
      " - output data tensor([-0.3835,  0.0405, -0.1957,  0.0760,  0.2833,  0.0710, -0.1207, -0.2396,\n",
      "        -0.0914,  0.1526,  0.4458, -0.2396, -0.1489,  0.1547, -0.2467,  0.0515,\n",
      "        -0.1017, -0.1289,  0.0077, -0.2561,  0.3177,  0.2572, -0.3113,  0.2059,\n",
      "        -0.0753, -0.0300,  0.1953, -0.0727,  0.1818,  0.1184,  0.0651, -0.3164,\n",
      "        -0.0672, -0.1193, -0.1503, -0.0989, -0.1155,  0.1072,  0.1429,  0.2492,\n",
      "        -0.1559, -0.1490, -0.1013, -0.2056,  0.0280,  0.0778,  0.0019,  0.2454,\n",
      "        -0.1603, -0.0739,  0.1541, -0.0605,  0.1128, -0.1654, -0.1146,  0.1593,\n",
      "         0.0544, -0.1781,  0.2510, -0.0615,  0.0607,  0.0775,  0.1169,  0.0742,\n",
      "        -0.1985,  0.0972,  0.1135,  0.0650, -0.0073, -0.0577, -0.2598,  0.3953,\n",
      "         0.0120,  0.2349,  0.3297, -0.0994,  0.0084,  0.0385, -0.1588,  0.1634,\n",
      "         0.2068,  0.0027, -0.3432,  0.2114,  0.0768, -0.1630,  0.0816,  0.0757,\n",
      "        -0.0581,  0.2104, -0.2734,  0.0071, -0.0058, -0.0991,  0.1467, -0.2069,\n",
      "        -0.1472,  0.0938, -0.1606, -0.5118,  0.0279,  0.0430, -0.0006, -0.1550,\n",
      "         0.0484, -0.0905,  0.2431,  0.2104, -0.0677, -0.0549, -0.1739, -0.1686],\n",
      "       grad_fn=<SelectBackward>)\n",
      " 0 plane data tensor([0.0000, 0.0000, 0.0000, 0.1509, 0.9402, 0.6295, 0.0176, 0.3278, 0.6304,\n",
      "        0.1663, 0.8415, 0.1030, 0.5810, 0.0945, 0.3517, 0.0452, 0.3174, 0.1949,\n",
      "        0.6220, 0.1984, 0.3751, 0.9955, 0.3739, 0.1206, 0.4264, 0.1790, 0.9629,\n",
      "        0.5087, 0.1203, 0.8852, 0.4748, 0.2879, 0.7333, 0.4865, 0.1402, 0.4241,\n",
      "        0.6778, 0.3414, 0.2684, 0.0153, 0.7225, 0.0465, 0.3701, 0.4924, 0.9972,\n",
      "        0.3048, 0.5041, 0.8497, 0.0620, 0.9871, 0.9783, 0.7782, 0.0722, 0.3465,\n",
      "        0.5362, 0.8709, 0.4996, 0.2830, 0.8581, 0.8319, 0.6603, 0.8281, 0.6989,\n",
      "        0.2207, 0.3492])\n",
      "torch.Size([1, 3, 230, 65])\n",
      " 0 output data tensor([-0.3835,  0.0405, -0.1957,  0.0760,  0.2833,  0.0710, -0.1207, -0.2396,\n",
      "        -0.0914,  0.1526,  0.4458, -0.2396, -0.1489,  0.1547, -0.2467,  0.0515,\n",
      "        -0.1017, -0.1289,  0.0077, -0.2561,  0.3177,  0.2572, -0.3113,  0.2059,\n",
      "        -0.0753, -0.0300,  0.1953, -0.0727,  0.1818,  0.1184],\n",
      "       grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 64, 112, 30])\n",
      " 1 plane data tensor([0.0000e+00, 8.3190e-01, 6.6031e-01, 8.2808e-01, 6.9888e-01, 2.2070e-01,\n",
      "        3.4924e-01, 8.7521e-01, 1.8198e-01, 7.6968e-01, 8.9894e-01, 8.5397e-01,\n",
      "        6.8809e-01, 9.4523e-02, 4.3997e-01, 3.9607e-01, 8.3872e-03, 1.4808e-02,\n",
      "        7.8272e-01, 8.7690e-01, 1.5037e-01, 8.5996e-01, 7.7921e-01, 5.1543e-01,\n",
      "        5.9001e-01, 5.8517e-02, 1.6269e-01, 4.7455e-01, 5.3517e-01, 5.9622e-01,\n",
      "        1.3580e-01, 1.1249e-01, 3.3316e-01, 1.1852e-01, 3.7903e-01, 4.0628e-01,\n",
      "        8.2985e-01, 9.0706e-01, 2.1196e-01, 8.8883e-01, 9.7382e-01, 9.2784e-01,\n",
      "        5.0497e-01, 8.3488e-01, 2.5588e-01, 6.2599e-01, 3.8899e-02, 8.2554e-01,\n",
      "        9.7194e-01, 4.0699e-01, 2.9608e-01, 8.3885e-01, 7.6296e-01, 5.3827e-01,\n",
      "        7.0693e-01, 2.8621e-01, 1.2195e-01, 9.6711e-01, 5.3602e-04, 5.4843e-01,\n",
      "        1.7697e-01, 2.8856e-02, 3.9190e-01])\n",
      "torch.Size([1, 3, 230, 63])\n",
      "1 output data tensor([-0.0786,  0.0651, -0.3164, -0.0672, -0.1193, -0.1503, -0.0989, -0.1155,\n",
      "         0.1072,  0.1429,  0.2492, -0.1559, -0.1490, -0.1013, -0.2056,  0.0280,\n",
      "         0.0778,  0.0019,  0.2454, -0.1603, -0.0739,  0.1541, -0.0605,  0.1128,\n",
      "        -0.1654, -0.1146,  0.1593,  0.0544, -0.1781], grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 64, 112, 29])\n",
      " 2 plane data tensor([0.0000e+00, 9.6711e-01, 5.3602e-04, 5.4843e-01, 1.7697e-01, 2.8856e-02,\n",
      "        3.9190e-01, 5.9670e-02, 4.5230e-01, 9.2800e-01, 9.3211e-02, 1.0267e-01,\n",
      "        2.0840e-01, 8.1723e-01, 6.2664e-02, 4.5246e-01, 7.4320e-01, 4.0748e-01,\n",
      "        3.6369e-01, 9.1750e-01, 2.7624e-01, 5.2549e-02, 9.3358e-01, 3.9317e-01,\n",
      "        4.3075e-01, 3.7489e-01, 1.1421e-01, 7.0649e-01, 2.9446e-01, 8.1633e-01,\n",
      "        3.4806e-01, 1.9906e-02, 9.5522e-01, 9.8853e-01, 4.4761e-01, 9.9027e-01,\n",
      "        5.5884e-01, 7.1895e-01, 4.3565e-01, 5.8336e-01, 9.9298e-01, 2.0295e-01,\n",
      "        4.4612e-01, 3.6332e-01, 8.3499e-01, 6.8588e-01, 8.9052e-02, 7.4130e-01,\n",
      "        4.2362e-01, 1.9561e-01, 7.2612e-01, 4.6932e-01, 6.1458e-01, 8.3057e-01,\n",
      "        4.9735e-01, 3.4275e-01, 9.0038e-01, 1.3636e-01, 1.4621e-02, 4.6450e-01,\n",
      "        3.0965e-01, 3.3253e-01, 5.2106e-01])\n",
      "torch.Size([1, 3, 230, 63])\n",
      "2 output data tensor([-0.3040,  0.2510, -0.0615,  0.0607,  0.0775,  0.1169,  0.0742, -0.1985,\n",
      "         0.0972,  0.1135,  0.0650, -0.0073, -0.0577, -0.2598,  0.3953,  0.0120,\n",
      "         0.2349,  0.3297, -0.0994,  0.0084,  0.0385, -0.1588,  0.1634,  0.2068,\n",
      "         0.0027, -0.3432,  0.2114,  0.0768, -0.1630], grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 64, 112, 29])\n",
      " 3 plane data tensor([0.0000, 0.1364, 0.0146, 0.4645, 0.3096, 0.3325, 0.5211, 0.5645, 0.0101,\n",
      "        0.7262, 0.1108, 0.6333, 0.3812, 0.2081, 0.3856, 0.5392, 0.7386, 0.2297,\n",
      "        0.4352, 0.5055, 0.2009, 0.9900, 0.3977, 0.6627, 0.7362, 0.2202, 0.5476,\n",
      "        0.3959, 0.4884, 0.3036, 0.1679, 0.1490, 0.3371, 0.9367, 0.4967, 0.5645,\n",
      "        0.2696, 0.6040, 0.6242, 0.1306, 0.7226, 0.7088, 0.1516, 0.0623, 0.5371,\n",
      "        0.7929, 0.8933, 0.2046, 0.7004, 0.1335, 0.5923, 0.1599, 0.4356, 0.3240,\n",
      "        0.2663, 0.8242, 0.8574, 0.0000, 0.0000, 0.0000])\n",
      "torch.Size([1, 3, 230, 60])\n",
      " 3 output data tensor([-0.2925,  0.0816,  0.0757, -0.0581,  0.2104, -0.2734,  0.0071, -0.0058,\n",
      "        -0.0991,  0.1467, -0.2069, -0.1472,  0.0938, -0.1606, -0.5118,  0.0279,\n",
      "         0.0430, -0.0006, -0.1550,  0.0484, -0.0905,  0.2431,  0.2104, -0.0677,\n",
      "        -0.0549, -0.1739, -0.1686], grad_fn=<SelectBackward>)\n",
      "torch.Size([1, 64, 112, 27])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "t4d = torch.rand(1, 3, 224, 224) #쉽게 이미지를 생각하자 (batch, channel, height, width)\n",
    "\n",
    "# print(t4d[0][0][0])\n",
    "\n",
    "# Split.\n",
    "tensor_list = torch.split(t4d, 56, dim=3)\n",
    "\n",
    "pad_first = (3, 0, 3, 3)\n",
    "pad_second = (1, 0, 3, 3)\n",
    "pad_third = (1, 0, 3, 3)\n",
    "pad_fourth = (1, 3, 3, 3)\n",
    "\n",
    "serial_conv1 = torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=0,\n",
    "                                bias=False)\n",
    "serial_conv = torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                bias=False)\n",
    "\n",
    "\n",
    "# serial_conv.weight = serial_conv1.weight\n",
    "\n",
    "serial_out = F.pad(t4d, (3, 3, 3, 3))\n",
    "print(\" - plane data\", serial_out[0][0][3])\n",
    "print(\" - output data\", serial_conv1(serial_out)[0][0][3])\n",
    "\n",
    "tensor_lis = torch.cat([tensor_list[0], tensor_list[1][:, :, :, :7-1]], dim=3)\n",
    "\n",
    "out = F.pad(tensor_lis, pad_first) # effectively zero padding, \n",
    "print(\" 0 plane data\", out[0][0][3])\n",
    "\n",
    "print(out.size())\n",
    "print(\" 0 output data\", serial_conv1(out)[0][0][3])\n",
    "print(serial_conv1(out).size()) # 따라서 마지막 차원인 width가 2 증가.\n",
    "\n",
    "tensor_lis1 = torch.cat([tensor_list[1], tensor_list[2][:, :, :, :7-1]], dim=3)\n",
    "\n",
    "out = F.pad(tensor_lis1, pad_second) # effectively zero padding, \n",
    "print(\" 1 plane data\", out[0][0][3])\n",
    "\n",
    "\n",
    "print(out.size()) # 따라서 마지막 차원인 width가 2 증가\n",
    "print(\"1 output data\", serial_conv1(out)[0][0][3])\n",
    "print(serial_conv1(out).size())\n",
    "\n",
    "tensor_lis2 = torch.cat([tensor_list[2], tensor_list[3][:, :, :, :7-1]], dim=3)\n",
    "\n",
    "out = F.pad(tensor_lis2, pad_third) # effectively zero padding, \n",
    "print(\" 2 plane data\", out[0][0][3])\n",
    "\n",
    "\n",
    "print(out.size()) # 따라서 마지막 차원인 width가 2 증가.\n",
    "print(\"2 output data\", serial_conv1(out)[0][0][3])\n",
    "print(serial_conv1(out).size())\n",
    "\n",
    "tensor_lis3 = tensor_list[3]\n",
    "\n",
    "out = F.pad(tensor_lis3, pad_fourth) # effectively zero padding, \n",
    "print(\" 3 plane data\", out[0][0][3])\n",
    "\n",
    "print(out.size()) # 따라서 마지막 차원인 width가 2 증가.\n",
    "print(\" 3 output data\", serial_conv1(out)[0][0][3])\n",
    "print(serial_conv1(out).size())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f70def66b0118ebfeb63e1939f6f3fe4acdf62107faa0ef990d3a8dee5d336cc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
