{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[  0,   1,   2,   3,   4],\n",
      "          [ 15,  16,  17,  18,  19],\n",
      "          [ 30,  31,  32,  33,  34],\n",
      "          [ 45,  46,  47,  48,  49],\n",
      "          [ 60,  61,  62,  63,  64],\n",
      "          [ 75,  76,  77,  78,  79],\n",
      "          [ 90,  91,  92,  93,  94],\n",
      "          [105, 106, 107, 108, 109],\n",
      "          [120, 121, 122, 123, 124],\n",
      "          [135, 136, 137, 138, 139],\n",
      "          [150, 151, 152, 153, 154],\n",
      "          [165, 166, 167, 168, 169],\n",
      "          [180, 181, 182, 183, 184],\n",
      "          [195, 196, 197, 198, 199],\n",
      "          [210, 211, 212, 213, 214]],\n",
      "\n",
      "         [[225, 226, 227, 228, 229],\n",
      "          [240, 241, 242, 243, 244],\n",
      "          [255, 256, 257, 258, 259],\n",
      "          [270, 271, 272, 273, 274],\n",
      "          [285, 286, 287, 288, 289],\n",
      "          [300, 301, 302, 303, 304],\n",
      "          [315, 316, 317, 318, 319],\n",
      "          [330, 331, 332, 333, 334],\n",
      "          [345, 346, 347, 348, 349],\n",
      "          [360, 361, 362, 363, 364],\n",
      "          [375, 376, 377, 378, 379],\n",
      "          [390, 391, 392, 393, 394],\n",
      "          [405, 406, 407, 408, 409],\n",
      "          [420, 421, 422, 423, 424],\n",
      "          [435, 436, 437, 438, 439]],\n",
      "\n",
      "         [[450, 451, 452, 453, 454],\n",
      "          [465, 466, 467, 468, 469],\n",
      "          [480, 481, 482, 483, 484],\n",
      "          [495, 496, 497, 498, 499],\n",
      "          [510, 511, 512, 513, 514],\n",
      "          [525, 526, 527, 528, 529],\n",
      "          [540, 541, 542, 543, 544],\n",
      "          [555, 556, 557, 558, 559],\n",
      "          [570, 571, 572, 573, 574],\n",
      "          [585, 586, 587, 588, 589],\n",
      "          [600, 601, 602, 603, 604],\n",
      "          [615, 616, 617, 618, 619],\n",
      "          [630, 631, 632, 633, 634],\n",
      "          [645, 646, 647, 648, 649],\n",
      "          [660, 661, 662, 663, 664]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.arange(1*3*15*15).view(1,3,15,15)\n",
    "# print(a)\n",
    "# print(a[:, 1:2])\n",
    "# print(torch.chunk(a, 4, dim=1)[0][:, :3, :])\n",
    "# # b = torch.ones(3, 4, 4)\n",
    "print(a[:, :, :, 0:5])\n",
    "# # c = torch.cat([a, b[:, :3, :]], dim=1)\n",
    "# # print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor([[[[  0,   1,   2,   3],\n",
      "          [ 15,  16,  17,  18],\n",
      "          [ 30,  31,  32,  33],\n",
      "          [ 45,  46,  47,  48],\n",
      "          [ 60,  61,  62,  63],\n",
      "          [ 75,  76,  77,  78],\n",
      "          [ 90,  91,  92,  93],\n",
      "          [105, 106, 107, 108],\n",
      "          [120, 121, 122, 123],\n",
      "          [135, 136, 137, 138],\n",
      "          [150, 151, 152, 153],\n",
      "          [165, 166, 167, 168],\n",
      "          [180, 181, 182, 183],\n",
      "          [195, 196, 197, 198],\n",
      "          [210, 211, 212, 213]],\n",
      "\n",
      "         [[225, 226, 227, 228],\n",
      "          [240, 241, 242, 243],\n",
      "          [255, 256, 257, 258],\n",
      "          [270, 271, 272, 273],\n",
      "          [285, 286, 287, 288],\n",
      "          [300, 301, 302, 303],\n",
      "          [315, 316, 317, 318],\n",
      "          [330, 331, 332, 333],\n",
      "          [345, 346, 347, 348],\n",
      "          [360, 361, 362, 363],\n",
      "          [375, 376, 377, 378],\n",
      "          [390, 391, 392, 393],\n",
      "          [405, 406, 407, 408],\n",
      "          [420, 421, 422, 423],\n",
      "          [435, 436, 437, 438]],\n",
      "\n",
      "         [[450, 451, 452, 453],\n",
      "          [465, 466, 467, 468],\n",
      "          [480, 481, 482, 483],\n",
      "          [495, 496, 497, 498],\n",
      "          [510, 511, 512, 513],\n",
      "          [525, 526, 527, 528],\n",
      "          [540, 541, 542, 543],\n",
      "          [555, 556, 557, 558],\n",
      "          [570, 571, 572, 573],\n",
      "          [585, 586, 587, 588],\n",
      "          [600, 601, 602, 603],\n",
      "          [615, 616, 617, 618],\n",
      "          [630, 631, 632, 633],\n",
      "          [645, 646, 647, 648],\n",
      "          [660, 661, 662, 663]]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ensure_divisibility(numerator, denominator):\n",
    "    \"\"\"Ensure that numerator is divisible by the denominator.\"\"\"\n",
    "    assert numerator % denominator == 0, '{} is not divisible by {}'.format(\n",
    "        numerator, denominator)\n",
    "\n",
    "\n",
    "def divide(numerator, denominator):\n",
    "    \"\"\"Ensure that numerator is divisible by the denominator and return\n",
    "    the division value.\"\"\"\n",
    "    # ensure_divisibility(numerator, denominator)\n",
    "    return numerator // denominator\n",
    "\n",
    "def split_tensor_along_last_dim(tensor, num_partitions,\n",
    "                                kernel_size,\n",
    "                                contiguous_split_chunks=False):\n",
    "    \"\"\"Split a tensor along its last dimension.\n",
    "    Arguments:\n",
    "        tensor: input tensor.\n",
    "        num_partitions: number of partitions to split the tensor\n",
    "        contiguous_split_chunks: If True, make each chunk contiguous\n",
    "                                 in memory.\n",
    "    \"\"\"\n",
    "    # Get the size and dimension.\n",
    "    last_dim = tensor.dim() - 1\n",
    "    last_dim_size = divide(tensor.size()[last_dim], num_partitions)\n",
    "    print(last_dim_size)\n",
    "    # Split.\n",
    "    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\n",
    "    tensor_custom = [torch.cat([tensor_list[i], tensor[:, :, :, ((i+1)*last_dim_size):((i+1)*last_dim_size)+kernel_size-1]], dim=last_dim) for i in range(num_partitions-1)]\n",
    "    tensor_custom.append(tensor_list[-1])\n",
    "\n",
    "    # Note: torch.split does not create contiguous tensors by default.\n",
    "    if contiguous_split_chunks:\n",
    "        return tuple(chunk.contiguous() for chunk in tensor_list)\n",
    "\n",
    "    return tensor_custom\n",
    "\n",
    "\n",
    "print(split_tensor_along_last_dim(a, 4, 2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27212/580500247.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27212/580500247.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m                                 bias=False)\n\u001b[0;32m     55\u001b[0m     parallel_conv = ParallelConv2d(3, 64, kernel_size=7, stride=2, padding=3,\n\u001b[1;32m---> 56\u001b[1;33m                                 bias=False)\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[0mserial_conv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mserial_conv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparallel_conv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Distribution_example\\model\\resnet.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, last_cnn, bias, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m                  padding=0, dilation=1, groups=1, last_cnn=False, bias=True, **kwargs):\n\u001b[0;32m     50\u001b[0m         super(ParallelConv2d, self).__init__(in_channels, out_channels, kernel_size, stride,\n\u001b[1;32m---> 51\u001b[1;33m                  padding, dilation, groups, bias, device=torch.cuda.current_device(), **kwargs)\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mngpus_per_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from model.resnet import ParallelConv2d\n",
    "from model.initialize import initialize_model_parallel\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "\n",
    "def test_conv(rank, ngpus_per_node, serial_conv, parallel_conv, input_data):\n",
    "    if rank():\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "\n",
    "    end.record()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    dist.init_process_group(backend=\"nccl\", init_method=\"tcp://127.0.0.1:6006\", world_size=ngpus_per_node, rank=rank)\n",
    "    initialize_model_parallel()\n",
    "    for p in parallel_conv.parameters():\n",
    "        print(\"[ rank {} ] parallel weight : \".format(str(rank)), p)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    parallel_conv.cuda(rank)\n",
    "    if rank == 0:\n",
    "        serial_conv = torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                bias=False).cuda(rank)\n",
    "        for p in serial_conv.parameters():\n",
    "            print(\"serial weight :\",  p)\n",
    "\n",
    "    parallel_result = parallel_conv(parallel_test_data)\n",
    "\n",
    "    if rank == 0:\n",
    "        print(\"[Device : {} ]\".format(str(torch.cuda.current_device())))\n",
    "        serial_result = serial_conv(parallel_test_data)\n",
    "        print(\"Parallel Result Shape : \", parallel_result.size())\n",
    "        print(\"Serial Result Shape : \", serial_result.size())\n",
    "        print(\"Parallel : \", parallel_result)\n",
    "        print(\"Serial : \", serial_result)\n",
    "        # parallel_result and serial_result should be the same\n",
    "        assert torch.allclose(parallel_result, serial_result), \"Parallel and Serial results are not the same\"\n",
    "        print(\"Conv layer Test Passed!!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 한 노드의 GPU수를 가져옴.\n",
    "    ngpus_per_node = torch.cuda.device_count()\n",
    "    input_data = torch.randn(1, 3, 224, 224)\n",
    "    # multiprocessing_distributed 변수가 true라면, world_size를 총 GPU개수로 설정한 후에, 메인 워커를 실행함.\n",
    "    serial_conv = torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                bias=False)\n",
    "    parallel_conv = ParallelConv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                bias=False)\n",
    "    serial_conv.weight = torch.ones((2,), dtype=torch.int8)\n",
    "    serial_conv.weight.new_tensor(parallel_conv.weight)\n",
    "    print(\"Are they same? : \", torch.allclose(serial_conv.weight, parallel_conv.weight))\n",
    "\n",
    "    #mp.spawn(test_conv, nprocs=ngpus_per_node, args=(ngpus_per_node, serial_conv, parallel_conv, input_data))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0753,  0.0312,  0.0084,  ...,  0.0498,  0.0002,  0.0824],\n",
      "          [ 0.0799, -0.0630, -0.0130,  ...,  0.0809,  0.0636,  0.0688],\n",
      "          [-0.0088, -0.0663,  0.0739,  ..., -0.0795, -0.0810,  0.0557],\n",
      "          ...,\n",
      "          [-0.0821, -0.0409, -0.0492,  ...,  0.0804,  0.0298,  0.0719],\n",
      "          [-0.0641, -0.0250,  0.0629,  ..., -0.0809,  0.0558, -0.0650],\n",
      "          [-0.0066,  0.0691,  0.0639,  ...,  0.0469,  0.0375,  0.0584]],\n",
      "\n",
      "         [[-0.0543,  0.0693, -0.0768,  ..., -0.0251,  0.0316,  0.0338],\n",
      "          [-0.0789, -0.0232, -0.0717,  ..., -0.0712,  0.0341, -0.0531],\n",
      "          [ 0.0211, -0.0073, -0.0616,  ..., -0.0237,  0.0784,  0.0252],\n",
      "          ...,\n",
      "          [-0.0634, -0.0019, -0.0616,  ...,  0.0016, -0.0012,  0.0531],\n",
      "          [-0.0738, -0.0161, -0.0709,  ...,  0.0176,  0.0616,  0.0707],\n",
      "          [-0.0629,  0.0606,  0.0364,  ..., -0.0052,  0.0097, -0.0705]],\n",
      "\n",
      "         [[-0.0606,  0.0284, -0.0641,  ..., -0.0329, -0.0711, -0.0482],\n",
      "          [-0.0144,  0.0051, -0.0031,  ...,  0.0489,  0.0684, -0.0280],\n",
      "          [-0.0779, -0.0698,  0.0780,  ..., -0.0729, -0.0202,  0.0125],\n",
      "          ...,\n",
      "          [-0.0600, -0.0429, -0.0787,  ..., -0.0012,  0.0110, -0.0792],\n",
      "          [ 0.0696,  0.0351,  0.0670,  ...,  0.0674, -0.0696,  0.0693],\n",
      "          [ 0.0491,  0.0807, -0.0357,  ..., -0.0152,  0.0789, -0.0502]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0070, -0.0438,  0.0733,  ...,  0.0528, -0.0365, -0.0734],\n",
      "          [ 0.0093,  0.0663, -0.0447,  ...,  0.0824,  0.0624, -0.0498],\n",
      "          [-0.0076, -0.0691,  0.0224,  ..., -0.0004,  0.0297, -0.0504],\n",
      "          ...,\n",
      "          [ 0.0630,  0.0117, -0.0241,  ..., -0.0299,  0.0371,  0.0612],\n",
      "          [-0.0765,  0.0474, -0.0385,  ..., -0.0455, -0.0346,  0.0312],\n",
      "          [ 0.0755, -0.0477, -0.0551,  ..., -0.0631, -0.0222, -0.0550]],\n",
      "\n",
      "         [[ 0.0263,  0.0443,  0.0143,  ..., -0.0612, -0.0054, -0.0249],\n",
      "          [-0.0442,  0.0221,  0.0303,  ...,  0.0473,  0.0815, -0.0460],\n",
      "          [-0.0173,  0.0102,  0.0124,  ...,  0.0491, -0.0419,  0.0258],\n",
      "          ...,\n",
      "          [-0.0493, -0.0550, -0.0387,  ...,  0.0466, -0.0211, -0.0056],\n",
      "          [ 0.0183, -0.0539, -0.0396,  ..., -0.0060, -0.0801,  0.0309],\n",
      "          [ 0.0239, -0.0412, -0.0550,  ..., -0.0325,  0.0700,  0.0420]],\n",
      "\n",
      "         [[-0.0544, -0.0313, -0.0085,  ..., -0.0770,  0.0052, -0.0316],\n",
      "          [ 0.0533,  0.0813,  0.0564,  ..., -0.0495, -0.0344,  0.0398],\n",
      "          [-0.0761,  0.0349, -0.0821,  ...,  0.0467,  0.0498, -0.0663],\n",
      "          ...,\n",
      "          [ 0.0446,  0.0573, -0.0716,  ...,  0.0744,  0.0515, -0.0654],\n",
      "          [-0.0101,  0.0679,  0.0175,  ...,  0.0663,  0.0030,  0.0660],\n",
      "          [-0.0771,  0.0517,  0.0301,  ...,  0.0587, -0.0809, -0.0769]]],\n",
      "\n",
      "\n",
      "        [[[-0.0273,  0.0210, -0.0511,  ...,  0.0775, -0.0285, -0.0287],\n",
      "          [ 0.0124,  0.0437,  0.0024,  ..., -0.0478, -0.0004,  0.0352],\n",
      "          [-0.0623, -0.0708, -0.0528,  ...,  0.0122, -0.0481,  0.0683],\n",
      "          ...,\n",
      "          [-0.0421, -0.0756, -0.0441,  ..., -0.0609,  0.0248, -0.0395],\n",
      "          [ 0.0720,  0.0598,  0.0767,  ..., -0.0381,  0.0397,  0.0716],\n",
      "          [ 0.0769, -0.0801,  0.0682,  ..., -0.0562, -0.0680,  0.0560]],\n",
      "\n",
      "         [[ 0.0050, -0.0125,  0.0010,  ...,  0.0614, -0.0020,  0.0166],\n",
      "          [ 0.0564, -0.0443,  0.0641,  ...,  0.0482, -0.0665, -0.0123],\n",
      "          [-0.0083, -0.0208,  0.0704,  ...,  0.0206, -0.0591,  0.0159],\n",
      "          ...,\n",
      "          [-0.0125, -0.0725, -0.0150,  ..., -0.0065,  0.0104,  0.0621],\n",
      "          [-0.0497,  0.0589,  0.0464,  ...,  0.0382, -0.0418, -0.0062],\n",
      "          [-0.0297,  0.0449, -0.0659,  ..., -0.0109, -0.0056, -0.0051]],\n",
      "\n",
      "         [[ 0.0018, -0.0066,  0.0030,  ...,  0.0623,  0.0763, -0.0276],\n",
      "          [ 0.0125,  0.0242, -0.0743,  ..., -0.0188,  0.0108,  0.0015],\n",
      "          [ 0.0236, -0.0600, -0.0169,  ...,  0.0324,  0.0048, -0.0126],\n",
      "          ...,\n",
      "          [ 0.0656, -0.0233,  0.0781,  ...,  0.0186, -0.0086, -0.0181],\n",
      "          [-0.0217, -0.0220,  0.0265,  ..., -0.0399,  0.0248,  0.0618],\n",
      "          [-0.0753,  0.0763,  0.0360,  ..., -0.0507, -0.0474,  0.0314]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0648,  0.0023,  0.0568,  ..., -0.0055, -0.0517, -0.0791],\n",
      "          [-0.0090, -0.0820,  0.0628,  ...,  0.0006, -0.0592,  0.0505],\n",
      "          [-0.0501,  0.0632,  0.0791,  ..., -0.0284, -0.0748,  0.0665],\n",
      "          ...,\n",
      "          [ 0.0078, -0.0764, -0.0137,  ...,  0.0405, -0.0193, -0.0410],\n",
      "          [ 0.0204, -0.0704, -0.0809,  ...,  0.0272,  0.0087, -0.0605],\n",
      "          [-0.0214, -0.0506, -0.0750,  ...,  0.0305,  0.0777,  0.0730]],\n",
      "\n",
      "         [[-0.0726, -0.0396, -0.0213,  ..., -0.0265,  0.0259,  0.0476],\n",
      "          [ 0.0516,  0.0684, -0.0652,  ...,  0.0022,  0.0758, -0.0130],\n",
      "          [ 0.0187,  0.0686,  0.0192,  ...,  0.0515, -0.0222, -0.0125],\n",
      "          ...,\n",
      "          [ 0.0533, -0.0311,  0.0129,  ..., -0.0669, -0.0129,  0.0691],\n",
      "          [ 0.0662, -0.0172,  0.0092,  ..., -0.0013,  0.0759, -0.0545],\n",
      "          [-0.0136, -0.0294,  0.0146,  ...,  0.0682,  0.0374, -0.0717]],\n",
      "\n",
      "         [[ 0.0531,  0.0586, -0.0276,  ...,  0.0616,  0.0673, -0.0164],\n",
      "          [ 0.0661,  0.0556, -0.0073,  ..., -0.0023, -0.0468,  0.0188],\n",
      "          [ 0.0705,  0.0517,  0.0316,  ...,  0.0058, -0.0019, -0.0205],\n",
      "          ...,\n",
      "          [-0.0080,  0.0022, -0.0144,  ...,  0.0377,  0.0168, -0.0715],\n",
      "          [ 0.0217,  0.0703,  0.0245,  ..., -0.0031,  0.0626,  0.0009],\n",
      "          [ 0.0348, -0.0333, -0.0138,  ...,  0.0129,  0.0497,  0.0701]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0661,  0.0534,  0.0116,  ..., -0.0667, -0.0437,  0.0804],\n",
      "          [ 0.0600, -0.0288, -0.0103,  ..., -0.0806, -0.0546, -0.0707],\n",
      "          [ 0.0366, -0.0642, -0.0322,  ..., -0.0538, -0.0483, -0.0720],\n",
      "          ...,\n",
      "          [ 0.0155,  0.0636,  0.0102,  ...,  0.0631, -0.0651,  0.0159],\n",
      "          [ 0.0643, -0.0764,  0.0570,  ...,  0.0596,  0.0207,  0.0538],\n",
      "          [-0.0045,  0.0770,  0.0428,  ...,  0.0460, -0.0378, -0.0712]],\n",
      "\n",
      "         [[-0.0572, -0.0667,  0.0459,  ...,  0.0259,  0.0462, -0.0340],\n",
      "          [ 0.0021,  0.0815, -0.0194,  ...,  0.0748, -0.0674, -0.0208],\n",
      "          [-0.0051, -0.0319,  0.0797,  ..., -0.0472,  0.0438, -0.0226],\n",
      "          ...,\n",
      "          [-0.0823,  0.0821,  0.0700,  ...,  0.0066, -0.0175,  0.0232],\n",
      "          [-0.0209, -0.0056,  0.0217,  ..., -0.0545, -0.0275,  0.0500],\n",
      "          [ 0.0755,  0.0030, -0.0066,  ...,  0.0361,  0.0311, -0.0556]],\n",
      "\n",
      "         [[-0.0467,  0.0400, -0.0131,  ...,  0.0618, -0.0050,  0.0182],\n",
      "          [-0.0480,  0.0406,  0.0547,  ...,  0.0113,  0.0021, -0.0515],\n",
      "          [ 0.0155, -0.0660,  0.0409,  ...,  0.0231,  0.0012,  0.0195],\n",
      "          ...,\n",
      "          [-0.0625,  0.0184, -0.0652,  ..., -0.0261,  0.0656,  0.0551],\n",
      "          [ 0.0238, -0.0305,  0.0625,  ..., -0.0566, -0.0819,  0.0101],\n",
      "          [-0.0364, -0.0798, -0.0263,  ..., -0.0681,  0.0527, -0.0685]]],\n",
      "\n",
      "\n",
      "        [[[-0.0341,  0.0722, -0.0575,  ..., -0.0333, -0.0796, -0.0618],\n",
      "          [ 0.0263,  0.0216,  0.0300,  ..., -0.0294,  0.0333, -0.0105],\n",
      "          [-0.0599, -0.0647, -0.0505,  ...,  0.0556,  0.0722,  0.0089],\n",
      "          ...,\n",
      "          [ 0.0492, -0.0652,  0.0283,  ..., -0.0673, -0.0212,  0.0794],\n",
      "          [ 0.0385,  0.0540,  0.0570,  ..., -0.0818, -0.0566, -0.0247],\n",
      "          [-0.0706,  0.0588,  0.0081,  ...,  0.0630,  0.0428, -0.0810]],\n",
      "\n",
      "         [[-0.0078,  0.0324,  0.0230,  ...,  0.0420,  0.0394,  0.0299],\n",
      "          [-0.0043, -0.0100, -0.0022,  ...,  0.0318,  0.0775, -0.0038],\n",
      "          [-0.0325,  0.0502, -0.0455,  ...,  0.0821,  0.0365, -0.0732],\n",
      "          ...,\n",
      "          [ 0.0498, -0.0608,  0.0715,  ..., -0.0614,  0.0593,  0.0262],\n",
      "          [ 0.0289, -0.0059, -0.0018,  ..., -0.0239,  0.0518, -0.0740],\n",
      "          [-0.0032,  0.0136, -0.0125,  ..., -0.0338, -0.0516,  0.0234]],\n",
      "\n",
      "         [[ 0.0591,  0.0621, -0.0115,  ..., -0.0339, -0.0593, -0.0736],\n",
      "          [-0.0810,  0.0524,  0.0024,  ..., -0.0393, -0.0544, -0.0523],\n",
      "          [-0.0597,  0.0021,  0.0659,  ...,  0.0265,  0.0652,  0.0474],\n",
      "          ...,\n",
      "          [ 0.0074, -0.0245, -0.0625,  ..., -0.0322, -0.0134, -0.0468],\n",
      "          [ 0.0362, -0.0026,  0.0425,  ..., -0.0092,  0.0056,  0.0681],\n",
      "          [ 0.0391,  0.0108, -0.0093,  ..., -0.0449, -0.0591, -0.0610]]]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "serial_conv = torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                bias=False)\n",
    "serial_conv1 = torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                bias=False)\n",
    "\n",
    "\n",
    "serial_conv.weight = serial_conv1.weight\n",
    "\n",
    "print(torch.allclose(serial_conv.weight, serial_conv1.weight))\n",
    "parallel_conv = torch.empty_like(serial_conv.weight).copy_(serial_conv.weight)\n",
    "\n",
    "print(torch.allclose(serial_conv.weight, serial_conv1.weight))\n",
    "print(serial_conv.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "t4d = torch.empty(3, 3, 4, 2) #쉽게 이미지를 생각하자 (batch, channel, height, width)\n",
    "p1d = (1, 1) # pad last dim by 1 on each side, width의 왼쪽 오른쪽에 pad 추가\n",
    "out = F.pad(t4d, p1d, \"constant\", 0) # effectively zero padding, \n",
    "print(out.size()) torch.Size([3, 3, 4, 4]) # 따라서 마지막 차원인 width가 2 증가.\n",
    "p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2),\n",
    "#width 왼쪽 오른쪽에 pad 1개씩 추가, height 위 아래에 pad 2개씩 추가.\n",
    "out = F.pad(t4d, p2d, \"constant\", 0) \n",
    "print(out.size()) torch.Size([3, 3, 8, 4])  \n",
    "# height에 총 4개의 pad가 들어갔으므로 8, width에 2개가 들어가서 4가 되었다.\n",
    "t4d = torch.empty(3, 3, 4, 2) \n",
    "p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3) \n",
    "#마지막으로 (3,3) 은 channel 방향 앞 뒤로 3개씩 pad가 들어갔다.\n",
    "out = F.pad(t4d, p3d, \"constant\", 0) \n",
    "print(out.size()) torch.Size([3, 9, 7, 3])\n",
    "#다른건 보지 않고 channel의 차원(dimension 1)을 보면 6이 증가해서 9가 된 것을 볼 수 있다."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f70def66b0118ebfeb63e1939f6f3fe4acdf62107faa0ef990d3a8dee5d336cc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
